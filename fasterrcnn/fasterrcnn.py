# -*- coding: utf-8 -*-
"""fasterRCNN.ipynb

Automatically generated by Colaboratory.
"""

import numpy as np 
import pandas as pd 
from bs4 import BeautifulSoup
import torchvision
from torchvision import transforms, datasets, models
import torch
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from PIL import Image
import matplotlib.pyplot as plt
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor
import matplotlib.patches as patches


class MaskDataset(object):
    def __init__(self, transforms):
        self.transforms = transforms
        # load all image files, sorting them to
        # ensure that they are aligned
        self.imgs = list(sorted(os.listdir("/content/drive/MyDrive/Projects/trainingDataset/images/")))
#       self.labels = list(sorted(os.listdir("/content/drive/MyDrive/Projects/trainingDataset/annotations")))

    def __getitem__(self, idx):
        # load images ad masks
        file_image = 'maksssksksss'+ str(idx) + '.png'
        file_label = 'maksssksksss'+ str(idx) + '.xml'
        img_path = os.path.join("/content/drive/MyDrive/Projects/trainingDataset/images/", file_image)
        label_path = os.path.join("/content/drive/MyDrive/Projects/trainingDataset/annotations/", file_label)
        img = Image.open(img_path).convert("RGB")
        #Generate Label
        target = generate_target(idx, label_path)
        
        if self.transforms is not None:
            img = self.transforms(img)

        return img, target

    def __len__(self):
        return len(self.imgs)

# bounding box
def generate_box(obj):
    
    xmin = int(obj.find('xmin').text)
    ymin = int(obj.find('ymin').text)
    xmax = int(obj.find('xmax').text)
    ymax = int(obj.find('ymax').text)
    
    return [xmin, ymin, xmax, ymax]

# three labels: without_mask:1, with_mask:2, mask_weared_incorrect:3
def generate_label(obj):
    if obj.find('name').text == "with_mask":
        return 2
    elif obj.find('name').text == "mask_weared_incorrect":
        return 3
    return 1    

# read file, extract boxes, labels, image_id
def generate_target(image_id, file): 
    with open(file) as f:
        data = f.read()
        soup = BeautifulSoup(data, 'xml')
        objects = soup.find_all('object')

        num_objs = len(objects)

        # Bounding boxes for objects
        # In coco format, bbox = [xmin, ymin, width, height]
        # In pytorch, the input should be [xmin, ymin, xmax, ymax]
        boxes = []
        labels = []
        for i in objects:
            boxes.append(generate_box(i))
            labels.append(generate_label(i))
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        # Labels 
        labels = torch.as_tensor(labels, dtype=torch.int64)
        # Tensorise img_id
        img_id = torch.tensor([image_id])
        # Annotation is in dictionary format
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = img_id
        
        return target

def collate_fn(batch):
    return tuple(zip(*batch))

def get_model_instance_segmentation(num_classes):
    # load an instance segmentation model pre-trained pre-trained on COCO
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
    # get number of input features for the classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)

    return model

data_transform = transforms.Compose([
        transforms.ToTensor(), 
    ])

dataset = MaskDataset(data_transform)
data_loader = torch.utils.data.DataLoader(
 dataset, batch_size=4, collate_fn=collate_fn, drop_last=True)

torch.cuda.is_available()

model = get_model_instance_segmentation(3)

# train
def train():
  num_epochs = 100
  model.to(device)
    
# parameters
  params = [p for p in model.parameters() if p.requires_grad]
  optimizer = torch.optim.SGD(params, lr=0.005,
                                momentum=0.9, weight_decay=0.0005)

  len_dataloader = len(data_loader)

  for epoch in range(num_epochs):
    model.train()
    i = 0    
    epoch_loss = 0
    for imgs, annotations in data_loader:
        i += 1
        imgs = list(img.to(device) for img in imgs)
        # print('imgs', imgs)
        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]
        loss_dict = model([imgs[0]], [annotations[0]])
        losses = sum(loss for loss in loss_dict.values())        

        optimizer.zero_grad()
        losses.backward()
        optimizer.step() 
        # print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')
        epoch_loss += losses
        evaluate(model, data_loader, device=device)
    print(f'epoch: {epoch}, Loss: {losses}')
  torch.save(model.state_dict(),'model.pt')


def plot_predict_image(img_tensor, annotation, block=True):
    
    fig,ax = plt.subplots(1)
    img = img_tensor.cpu().data

    # Display the image
    ax.imshow( np.array( img.permute(1, 2, 0) ) )
    
    for box, label in zip( annotation["boxes"], annotation["labels"] ):
        # print("label",label)
        # print("box",box)
        xmin, ymin, xmax, ymax = box
        # Create a Rectangle patch
        if label==1:
          rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='red',facecolor='none')
          ax.text((xmin+xmax)/2-16,ymin-1, 'without mask', c='red')
        elif label==2:
          rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='blue',facecolor='none')
          ax.text((xmin+xmax)/2-2,ymin-1, 'with mask', c = 'blue')
        else:
          rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='yellow',facecolor='none')
          ax.text((xmin+xmax)/2-2,ymin-1, 'incorrect mask', c = 'yellow')

                    # Add the patch to the Axes
        ax.add_patch(rect)
        ax.axis("off")
    plt.show(block=block)

def plot_annotated_image(img_tensor, annotation, block=True):
    
    fig,ax = plt.subplots(1)
    img = img_tensor.cpu().data

    # Display the image
    ax.imshow( np.array( img.permute(1, 2, 0) ) )
    
    i = 0
    for box, label in zip( annotation["boxes"], annotation["labels"] ):
        # print("label",label)
        # print("box",box)
        
        score = annotation['scores'][i]
        if annotation['scores'][i] > 0:
          xmin, ymin, xmax, ymax = box
        # Create a Rectangle patch
          if label==1:  
            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='red',facecolor='none') 
            ax.text((xmin+xmax)/2-20,ymin-1, 'without mask{:.3f}'.format(score), c='red')
            print('red', score)
          elif label==2:  
            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='blue',facecolor='none')
            ax.text((xmin+xmax)/2-20,ymin-1, 'with mask {:.3f}'.format(score), c = 'blue')
            print('blue', score)
          else: 
            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='yellow',facecolor='none')
            ax.text((xmin+xmax)/2-20,ymin-1, 'incorrect mask {:.3f}'.format(score), c = 'yellow')
            print(score)

           # Add the patch to the Axes
          ax.add_patch(rect)
          ax.axis("off")
          i +=1
    plt.show(block=block)


def predict(image, model, device, detection_threshold):
    # transform the image to tensor
    coco_names = ['with-mask', 'without-mask', 'not wear correctly']
    outputs = model(image) # get the predictions on the image
    # print the results individually
    # print(f"BOXES: {outputs[0]['boxes']}")
    # print(f"LABELS: {outputs[0]['labels']}")
    # print(f"SCORES: {outputs[0]['scores']}")
    # get all the predicited class names
    pred_classes = [coco_names[i] for i in outputs[2]['labels'].cpu().numpy()]
    print('pred_classes', pred_classes)
    # get score for all the predicted objects
    pred_scores = outputs[0]['scores'].detach().cpu().numpy()
    print('predict_scores', pred_scores)
    # get all the predicted bounding boxes
    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()
    # get boxes above the threshold score
    boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)
    return boxes, pred_classes, outputs[0]['labels']

model = get_model_instance_segmentation(3)
  model.load_state_dict(torch.load('model.pt'))
  model.eval()
  model.to(device)
